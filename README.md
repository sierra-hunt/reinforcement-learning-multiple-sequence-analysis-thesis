Multiple sequence alignment (MSA) remains a fundamental yet computationally intensive challenge in bioinformatics, with ap- plications spanning phylogenetic analysis to protein structure prediction. While reinforcement learning (RL) has emerged as a promising approach for MSA, existing methods universally de- pend on the sum-of-pairs (SP) scoreâ€”a theoretically unsubstanti- ated metric criticized for overlooking evolutionary relationships. This study investigates whether RL agents can learn biologi- cally plausible alignment strategies without SP-score dependence, and examines the implicit relationship between learned policies and SP metrics. We develop a flexible RL framework with nine distinct reward functions, including SP-based, true-alignment, and biochemically informed alternatives. Through curriculum- based training on synthetic data, we demonstrate that agents using true-alignment and BLOSUM-based rewards outperform SP-dependent methods by 12-15% in perfect match rates while maintaining high SP scores (p < 0.01). State-correlation analyses reveal that non-SP agents implicitly leverage amino acid composi- tion (r = 0.63) and gap patterns (r = 0.58) resembling SP heuristics. These findings challenge the necessity of SP scores in RL-based MSA and suggest that agents can learn evolutionarily coherent strategies through alternative reward designs.
